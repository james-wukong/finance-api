{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a42676a7-9e18-4b8c-8393-172149346e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import pipeline, AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import col, to_date, to_timestamp\n",
    "from pyspark.errors import PySparkException\n",
    "from pymongo import MongoClient\n",
    "from pymongo import errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9077305-3941-41ad-898a-b3b26ec72e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mongo_uri(mongo_conf) -> str | None:\n",
    "    \"\"\"\n",
    "    generate mongo connection uri based on input\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not mongo_conf:\n",
    "        return None\n",
    "    return (f\"mongodb+srv://{mongo_conf['user']}:\"\n",
    "            f\"{mongo_conf['token']}@\"\n",
    "            f\"{mongo_conf['host']}\"\n",
    "            f\"/?retryWrites=true&w=majority\")\n",
    "\n",
    "def get_property(conf) -> dict:\n",
    "    \"\"\"\n",
    "    get property for database\n",
    "    \"\"\"\n",
    "    return {key: conf[key] for key in conf.keys()\n",
    "                         & {'user', 'password', 'driver'}}\n",
    "\n",
    "def gen_maria_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for maria database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:mysql://{conf['host']}:\"\n",
    "            f\"{conf['port']}/\"\n",
    "            f\"{db}?permitMysqlScheme\")\n",
    "\n",
    "def gen_postgres_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for maria database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:postgresql://{conf['host']}:\"\n",
    "            f\"{conf['port']}/{db}\")\n",
    "\n",
    "def gen_mssql_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for maria database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:sqlserver://{conf['host']}:{conf['port']};\"\n",
    "            f\"databaseName={db};encrypt=true;trustServerCertificate=true;\")\n",
    "\n",
    "def gen_azure_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for azure sql database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:sqlserver://{conf['host']}:{conf['port']};\"\n",
    "            f\"databaseName={db};encrypt=true;\")\n",
    "\n",
    "\n",
    "def init_mongodb_client(uri: str) -> MongoClient | None:\n",
    "    \"\"\"\n",
    "    initialize the mongo client\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize new MongoDB client\n",
    "        client = MongoClient(uri)\n",
    "    except errors.ConnectionFailure as e:\n",
    "        # Handle connection failure gracefully\n",
    "        print(f\"Failed to connect to MongoDB: {e}\")\n",
    "\n",
    "        return None\n",
    "    else:\n",
    "        return client\n",
    "\n",
    "def prepare_dataframe(spark, sc, data) -> DataFrame:\n",
    "    \"\"\"\n",
    "    prepare dataframe when the data might be in different data types\n",
    "    :param spark: spark session\n",
    "    :param sc: spark context\n",
    "    :param data: data to be processed\n",
    "    :return: spark sql dataframe\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame) and not data.empty:\n",
    "        print('data is pandas df and not empty')\n",
    "        df = spark.createDataFrame(data)\n",
    "    elif isinstance(data, DataFrame) and not data.isEmpty():\n",
    "        print('data is spark df and not empty')\n",
    "        df = data\n",
    "    elif isinstance(data, list):\n",
    "        print('data is list and not empty')\n",
    "        df = spark.read.json(sc.parallelize([json.dumps(record) for record in data]))\n",
    "    elif isinstance(data, dict):\n",
    "        print('data is dictionary and not empty')\n",
    "        data = [data]\n",
    "        df = spark.read.json(sc.parallelize([json.dumps(record) for record in data]))\n",
    "    else:\n",
    "        \n",
    "        print('data empty')\n",
    "        # initialize empty dataframe\n",
    "        schema = StructType([])\n",
    "        df = spark.createDataFrame([], schema)\n",
    "\n",
    "    return df\n",
    "    \n",
    "def write_to_mongo(spark, data, \n",
    "                   uri: str, db: str, col: str) -> None:\n",
    "    \"\"\"\n",
    "    write data into mongo db\n",
    "    :param spark: sparkSession\n",
    "    :param data: \n",
    "    :param uri: str, \n",
    "    :param db: str, \n",
    "    :param col: str, \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sc = spark.sparkContext\n",
    "    df = prepare_dataframe(spark, sc, data)\n",
    "    config = {\n",
    "        'uri': uri,\n",
    "        'database': db,\n",
    "        'collection': col\n",
    "    }\n",
    "    if not df.isEmpty():\n",
    "        df.write.format(\"mongo\") \\\n",
    "            .options(**config) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "\n",
    "def write_to_database(spark, data, conf,\n",
    "                   db: str, write_table: str, type: str = 'mariadb') -> None:\n",
    "    \"\"\"\n",
    "    write data into database table\n",
    "    :param data:\n",
    "    :param uri:\n",
    "    :param db:\n",
    "    :param table:\n",
    "    :param type:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sc = spark.sparkContext\n",
    "    df = prepare_dataframe(spark, sc, data)\n",
    "    properties = get_property(conf)\n",
    "\n",
    "    jdbc = ''\n",
    "    if type == 'mariadb':\n",
    "        jdbc = gen_maria_jdbc(conf, db)\n",
    "    elif type == 'postgres':\n",
    "        jdbc = gen_postgres_jdbc(conf, db)\n",
    "    elif type == 'mssql':\n",
    "        jdbc = gen_maria_jdbc(conf, db)\n",
    "    else:\n",
    "        raise ValueError('error when generating jdbc')\n",
    "\n",
    "    print(jdbc)\n",
    "\n",
    "    if not df.isEmpty():\n",
    "        print('writing data to', type)\n",
    "        df.write.jdbc(\n",
    "            url=jdbc,\n",
    "            table=write_table,\n",
    "            mode=\"append\",\n",
    "            properties=properties\n",
    "        )\n",
    "    else:\n",
    "        print('empty dataset')\n",
    "\n",
    "def sentiment_analysis(data, tokenizer, model) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    sentiment analysis and retrun dataframe with score\n",
    "    \"\"\"\n",
    "    data[\"Sentiment\"] = \"\"\n",
    "    sentiment = []\n",
    "    for i in data['text']:\n",
    "        tokenized_news = tokenizer(i, return_tensors=\"tf\")\n",
    "        logits = model.predict(tokenized_news).logits\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "        sentiment.append(probabilities)\n",
    "    data[\"Sentiment\"] = sentiment\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75227805-e4bb-4338-b79f-fa6007347811",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('conf.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "mongo_uri = gen_mongo_uri(config['mongodb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19ec3fb2-0945-4a1c-93fc-89ccf1c1c072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/sutring/.ivy2/cache"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # getting the spark instance\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName('Big Data Project ETL') \\\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
    "        .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
    "        .config(\"spark.jars.packages\", \"com.microsoft.azure:spark-mssql-connector_2.12:1.2.0,\"\n",
    "                                       \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .getOrCreate()\n",
    "except PySparkException as e:\n",
    "    print(f\"Failed to get or create Spark: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98443e4-a1cf-47cc-bccd-af0a1b4e07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_symbols = [\n",
    "        'AAPL', 'MSFT', 'NVDA', 'META', 'AMZN', 'TSLA', 'GOOGL',\n",
    "        'ON', 'DBD', 'DSGX', 'GTLB', 'LOGI', 'CRSR',\n",
    "        'LNG', 'SWN', 'APA', 'BTU', 'CL',\n",
    "        'BMY', 'THC', 'TNDM',\n",
    "        'MOS', 'AXTA', 'KOP',\n",
    "        'SBLK', 'EME', 'DNOW',\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d283284c-4842-48b8-80f9-3bf0666d4698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+------+--------------------+--------------------+\n",
      "|               date|       source|symbol|                text|               title|\n",
      "+-------------------+-------------+------+--------------------+--------------------+\n",
      "|2024-02-23 09:14:00|Seeking Alpha|  AAPL|Looking for stock...|Wedbush sees 1995...|\n",
      "|2024-02-23 08:41:00|     TipRanks|  AAPL|Looking for stock...|Apple (NASDAQ:AAP...|\n",
      "|2024-02-23 08:35:00|        Yahoo|  AAPL|AAPL, COST, WSM, ...|Zacks Market Edge...|\n",
      "|2024-02-23 08:23:00|  MarketWatch|  AAPL|Looking for stock...|Two years after U...|\n",
      "|2024-02-23 08:09:00|     DowJones|  AAPL|The actions of ma...|Two years after U...|\n",
      "+-------------------+-------------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# news option 1\n",
    "# read from maria - FINNHUB api\n",
    "# company news\n",
    "maria_fin_news_df = spark.read.jdbc(\n",
    "    url=gen_maria_jdbc(config['mariadb']),\n",
    "    table=f\"(SELECT FROM_UNIXTIME(datetime) as date, source, related as symbol, summary as text, \"\n",
    "              f\"headline as title FROM finn_company_news) t\",\n",
    "    properties=get_property(config['mariadb'])\n",
    ")\n",
    "maria_fin_news_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ef8bcd-ac9b-4d5f-b75a-0a3600765682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------+--------------------+--------------------+\n",
      "|               date|              source|symbol|                text|               title|\n",
      "+-------------------+--------------------+------+--------------------+--------------------+\n",
      "|2024-02-28 09:11:13|Zacks Investment ...|  PDCO|Patterson Cos. (P...|Patterson Cos. (P...|\n",
      "|2024-02-28 09:11:12|Zacks Investment ...|  EDIT|Editas Medicine (...|Editas Medicine (...|\n",
      "|2024-02-28 09:11:11|Zacks Investment ...|  NOVT|Novanta (NOVT) ca...|Novanta (NOVT) Me...|\n",
      "|2024-02-28 09:10:09| Proactive Investors| VRBFF|VanadiumCorp Reso...|VanadiumCorp Reso...|\n",
      "|2024-02-28 09:09:42|     The Motley Fool|  NVDA|Micron Technology...|Nvidia Stock Inve...|\n",
      "+-------------------+--------------------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# news option 2\n",
    "# read from postgresql - FMP api\n",
    "# stock news\n",
    "postgre_fmp_news_df = spark.read.jdbc(\n",
    "    url=gen_postgres_jdbc(config['postgres']),\n",
    "    # table=f\"(SELECT publishedDate as date, title, symbol, site as source, \"\n",
    "    #           f\"text FROM fmp_stock_news) t\",\n",
    "    table='fmp_stock_news',\n",
    "    properties=get_property(config['postgres'])\n",
    ")\n",
    "postgre_fmp_news_df = postgre_fmp_news_df.withColumnRenamed(\"publishedDate\", \"date\")\\\n",
    "                                        .withColumn(\"date\", to_timestamp(col(\"date\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                                        .withColumnRenamed(\"site\", \"source\") \\\n",
    "                                        .drop(\"image\").drop(\"url\")\n",
    "postgre_fmp_news_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a9638a7-fa30-4f84-bca2-a9a03c52da42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------+--------------------+--------------------+\n",
      "|               date|              source|symbol|                text|               title|\n",
      "+-------------------+--------------------+------+--------------------+--------------------+\n",
      "|2024-02-28 09:11:13|Zacks Investment ...|  PDCO|Patterson Cos. (P...|Patterson Cos. (P...|\n",
      "|2024-02-28 09:11:12|Zacks Investment ...|  EDIT|Editas Medicine (...|Editas Medicine (...|\n",
      "|2024-02-28 09:11:11|Zacks Investment ...|  NOVT|Novanta (NOVT) ca...|Novanta (NOVT) Me...|\n",
      "|2024-02-28 09:10:09| Proactive Investors| VRBFF|VanadiumCorp Reso...|VanadiumCorp Reso...|\n",
      "|2024-02-28 09:09:42|     The Motley Fool|  NVDA|Micron Technology...|Nvidia Stock Inve...|\n",
      "+-------------------+--------------------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing purpose\n",
    "# union news from different sources (different API)\n",
    "news_unioned_df = postgre_fmp_news_df.unionAll(maria_fin_news_df)\n",
    "news_unioned_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f910e09c-da8e-41a2-a90c-95488334542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from mysql for historical data\n",
    "# each stock will be saved in dividual table respectively after join other data columns\n",
    "data = {}\n",
    "for symbol in stock_symbols:\n",
    "    maria_hist_data_df = spark.read.jdbc(\n",
    "        url=gen_maria_jdbc(config['mariadb']),\n",
    "        table=f'(SELECT Date, Close, Volume from yf_historical_data WHERE symbol=\"{symbol}\") t',\n",
    "        properties=get_property(config['mariadb'])\n",
    "    )\n",
    "    data[symbol] = maria_hist_data_df.withColumnRenamed(\"Close\",symbol) \\\n",
    "                                    .withColumnRenamed(\"Volume\",f'{symbol}_vol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf06d7c1-ea05-4040-b54e-bfcd1ad8940f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+--------+\n",
      "|               Date|              NVDA|NVDA_vol|\n",
      "+-------------------+------------------+--------+\n",
      "|2004-02-26 00:00:00|1.8875000476837158|34497600|\n",
      "|2004-02-27 00:00:00|1.8541669845581055|59554800|\n",
      "|2004-03-01 00:00:00|1.8816670179367065|50662800|\n",
      "+-------------------+------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test if data is constructed correctly\n",
    "data[\"NVDA\"].show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e558f54-7698-463d-87b6-a7663a0856eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|                 _id|                data|symbol|\n",
      "+--------------------+--------------------+------+\n",
      "|{65de41856693865d...|[{-1852, , 2024-0...|  AAPL|\n",
      "|{65de41ab6693865d...|[{-300, , 2024-02...|  MSFT|\n",
      "|{65de41ce6693865d...|[{-36000, , 2024-...|  NVDA|\n",
      "|{65de41f16693865d...|[{-31493, , 2024-...|  META|\n",
      "|{65de42136693865d...|[{-500, , 2024-02...|  AMZN|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load inside transactions from mongodb\n",
    "mongo_df = spark.read.format('mongo') \\\n",
    "    .option(\"database\", 'finance_api') \\\n",
    "    .option(\"collection\", 'finn_insider_transactions') \\\n",
    "    .load()\n",
    "\n",
    "mongo_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e4878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model and tokenizer\n",
    "pipe = pipeline(\"text-classification\", model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce68bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "news_unioned_df = news_unioned_df.toPandas()\n",
    "news_sentiment = sentiment_analysis(news_unioned_df)\n",
    "news_df = news_sentiment[[\"symbol\", \"Sentiment\"]].sort_values(by=\"symbol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append sentiment score to historical data\n",
    "news_symbols = news_df[\"symbol\"].unique()\n",
    "data_news = {}\n",
    "for i in news_symbols:\n",
    "    news = []\n",
    "    news_for_symbol = news_df[news_df[\"symbol\"] == i]\n",
    "    for sentiment in news_for_symbol[\"Sentiment\"]:\n",
    "        news.append(sentiment)\n",
    "    data_news[i] = news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b827fc85-4076-4800-b7f8-682e9f8671cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data is spark df and not empty\n",
      "jdbc:mysql://13.92.123.83:3306/finance_out?permitMysqlScheme\n",
      "writing data to mariadb\n"
     ]
    }
   ],
   "source": [
    "# append result data from sentiment analysis model\n",
    "write_to_database(spark, data[\"NVDA\"], config['mariadb'],\n",
    "                   db='finance_out', write_table='historical_with_sentiment', type = 'mariadb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "714de28c-ff69-41d5-9c91-1dd770da163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec1e8a-f566-43c8-afa3-e9f37e17caa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
