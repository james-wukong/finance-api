{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import pipeline, AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import col, to_date, to_timestamp, explode\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.errors import PySparkException\n",
    "from pymongo import MongoClient\n",
    "from pymongo import errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mongo_uri(mongo_conf) -> str | None:\n",
    "    \"\"\"\n",
    "    generate mongo connection uri based on input\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not mongo_conf:\n",
    "        return None\n",
    "    return (f\"mongodb+srv://{mongo_conf['user']}:\"\n",
    "            f\"{mongo_conf['token']}@\"\n",
    "            f\"{mongo_conf['host']}\"\n",
    "            f\"/?retryWrites=true&w=majority\")\n",
    "\n",
    "def get_property(conf) -> dict:\n",
    "    \"\"\"\n",
    "    get property for database\n",
    "    \"\"\"\n",
    "    return {key: conf[key] for key in conf.keys()\n",
    "                         & {'user', 'password', 'driver'}}\n",
    "\n",
    "def gen_maria_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for maria database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:mysql://{conf['host']}:\"\n",
    "            f\"{conf['port']}/\"\n",
    "            f\"{db}?permitMysqlScheme\")\n",
    "\n",
    "def gen_postgres_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for maria database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:postgresql://{conf['host']}:\"\n",
    "            f\"{conf['port']}/{db}\")\n",
    "\n",
    "def gen_mssql_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for maria database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:sqlserver://{conf['host']}:{conf['port']};\"\n",
    "            f\"databaseName={db};encrypt=true;trustServerCertificate=true;\")\n",
    "\n",
    "def gen_azure_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for azure sql database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:sqlserver://{conf['host']}:{conf['port']};\"\n",
    "            f\"databaseName={db};encrypt=true;\")\n",
    "\n",
    "\n",
    "def init_mongodb_client(uri: str) -> MongoClient | None:\n",
    "    \"\"\"\n",
    "    initialize the mongo client\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize new MongoDB client\n",
    "        client = MongoClient(uri)\n",
    "    except errors.ConnectionFailure as e:\n",
    "        # Handle connection failure gracefully\n",
    "        print(f\"Failed to connect to MongoDB: {e}\")\n",
    "\n",
    "        return None\n",
    "    else:\n",
    "        return client\n",
    "\n",
    "def prepare_dataframe(spark, sc, data) -> DataFrame:\n",
    "    \"\"\"\n",
    "    prepare dataframe when the data might be in different data types\n",
    "    :param spark: spark session\n",
    "    :param sc: spark context\n",
    "    :param data: data to be processed\n",
    "    :return: spark sql dataframe\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame) and not data.empty:\n",
    "        df = spark.createDataFrame(data)\n",
    "    elif isinstance(data, DataFrame) and not data.isEmpty():\n",
    "        df = data\n",
    "    elif isinstance(data, list):\n",
    "        df = spark.read.json(sc.parallelize([json.dumps(record) for record in data]))\n",
    "    elif isinstance(data, dict):\n",
    "        data = [data]\n",
    "        df = spark.read.json(sc.parallelize([json.dumps(record) for record in data]))\n",
    "    else:\n",
    "        # initialize empty dataframe\n",
    "        schema = StructType([])\n",
    "        df = spark.createDataFrame([], schema)\n",
    "\n",
    "    return df\n",
    "    \n",
    "def write_to_mongo(spark, data, \n",
    "                   uri: str, db: str, col: str) -> None:\n",
    "    \"\"\"\n",
    "    write data into mongo db\n",
    "    :param spark: sparkSession\n",
    "    :param data: \n",
    "    :param uri: str, \n",
    "    :param db: str, \n",
    "    :param col: str, \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sc = spark.sparkContext\n",
    "    df = prepare_dataframe(spark, sc, data)\n",
    "    config = {\n",
    "        'uri': uri,\n",
    "        'database': db,\n",
    "        'collection': col\n",
    "    }\n",
    "    if not df.isEmpty():\n",
    "        df.write.format(\"mongo\") \\\n",
    "            .options(**config) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "\n",
    "def write_to_database(spark, data, conf,\n",
    "                   db: str, write_table: str, type: str = 'mariadb') -> None:\n",
    "    \"\"\"\n",
    "    write data into database table\n",
    "    :param data:\n",
    "    :param uri:\n",
    "    :param db:\n",
    "    :param table:\n",
    "    :param type:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sc = spark.sparkContext\n",
    "    df = prepare_dataframe(spark, sc, data)\n",
    "    properties = get_property(conf)\n",
    "\n",
    "    jdbc = ''\n",
    "    if type == 'mariadb':\n",
    "        jdbc = gen_maria_jdbc(conf, db)\n",
    "    elif type == 'postgres':\n",
    "        jdbc = gen_postgres_jdbc(conf, db)\n",
    "    elif type == 'mssql':\n",
    "        jdbc = gen_mssql_jdbc(conf, db)\n",
    "    elif type == 'azure':\n",
    "        jdbc = gen_azure_jdbc(conf, db)\n",
    "    else:\n",
    "        raise ValueError('error when generating jdbc')\n",
    "\n",
    "    if not df.isEmpty():\n",
    "        df.write.jdbc(\n",
    "            url=jdbc,\n",
    "            table=write_table,\n",
    "            mode=\"append\",\n",
    "            properties=properties\n",
    "        )\n",
    "    else:\n",
    "        print('empty dataset')\n",
    "\n",
    "def sentiment_analysis(data, tokenizer, model) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    sentiment analysis and retrun dataframe with score\n",
    "    \"\"\"\n",
    "    data['sentiment'] = None\n",
    "    for idx, text in enumerate(data['text']):\n",
    "        tokenized_news = tokenizer(text, truncation=True, return_tensors=\"tf\")\n",
    "        logits = model.predict(tokenized_news).logits\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "        data.at[idx, 'sentiment'] = probabilities.numpy()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('conf.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "mongo_uri = gen_mongo_uri(config['mongodb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # getting the spark instance\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName('Big Data Project ETL') \\\n",
    "        .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
    "        .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
    "        .config(\"spark.jars.packages\", \"com.microsoft.azure:spark-mssql-connector_2.12:1.2.0,\"\n",
    "                                       \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "except PySparkException as e:\n",
    "    print(f\"Failed to get or create Spark: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exception Caught: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read historical data with sentiment from mariadb\n",
    "historical_stock_df = spark.read.jdbc(\n",
    "    url=gen_maria_jdbc(config['mariadb'], 'finance_out'),\n",
    "    table='historical_with_sentiment',\n",
    "    properties=get_property(config['mariadb'])\n",
    ")\n",
    "cot_report_df = spark.read.jdbc(\n",
    "    url=gen_mssql_jdbc(config['mssql'], 'finance_api'),\n",
    "    table='fmp_income_stmt',\n",
    "    properties=get_property(config['mssql'])\n",
    ")\n",
    "income_stmt_df = spark.read.jdbc(\n",
    "    url=gen_postgres_jdbc(config['postgres'], 'finance_api'),\n",
    "    table='fmp_cmtmt_trader_report',\n",
    "    properties=get_property(config['postgres'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------------+----------+---------+\n",
      "|symbol|      date|              Close|    Volume|sentiment|\n",
      "+------+----------+-------------------+----------+---------+\n",
      "|  AAPL|2004-03-03|0.42714300751686096| 225131200|     NULL|\n",
      "|  AAPL|2004-03-04|0.44928601384162903| 660223200|     NULL|\n",
      "|  AAPL|2004-03-05|0.47749999165534973|1540599200|     NULL|\n",
      "|  AAPL|2004-03-08| 0.4642859995365143| 522872000|     NULL|\n",
      "|  AAPL|2004-03-09|0.48392900824546814| 618363200|     NULL|\n",
      "+------+----------+-------------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "historical_stock_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique symbol\n",
    "stock_symbols = historical_stock_df.select(\"symbol\").distinct().rdd.flatMap(lambda x : x).collect()\n",
    "sec_symbols = cot_report_df.select(\"symbol\").distinct().rdd.flatMap(lambda x : x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save historical data into mssql or azure, each stock as a table\n",
    "for symbol in stock_symbols:\n",
    "    stock_data = historical_stock_df.filter(historical_stock_df['symbol'] == symbol) \\\n",
    "                                .select('date', 'Close', 'sentiment') \\\n",
    "                                .withColumnRenamed('Close', symbol)\n",
    "    try:\n",
    "        # save with 'stock_' as prefix\n",
    "        write_to_database(spark, stock_data, config['mssql'],\n",
    "                          db='finance_out', write_table=f'stock_{symbol}', type='mssql')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save commitment report data into mssql or azure, each security as a table\n",
    "for symbol in sec_symbols:\n",
    "    cot_data = cot_report_df.filter(cot_report_df['symbol'] == symbol) \n",
    "    try:\n",
    "        # save with 'stock_' as prefix\n",
    "        write_to_database(spark, cot_data, config['mssql'],\n",
    "                          db='finance_out', write_table=f'cot_report_{symbol}', type='mssql')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save income statement data into mssql or azure, each stock as a table\n",
    "for symbol in stock_symbols:\n",
    "    income_stmt_data = income_stmt_df.filter(income_stmt_df['symbol'] == symbol)\n",
    "    try:\n",
    "        # save with 'stock_' as prefix\n",
    "        write_to_database(spark, income_stmt_data, config['mssql'],\n",
    "                          db='finance_out', write_table=f'income_stmt_{symbol}', type='mssql')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|                 _id|                data|symbol|\n",
      "+--------------------+--------------------+------+\n",
      "|{65e550a0b142680e...|[{-100000, , 2024...|  AAPL|\n",
      "|{65e550bab142680e...|[{-300, , 2024-02...|  MSFT|\n",
      "|{65e550d4b142680e...|[{-5000, , 2024-0...|  NVDA|\n",
      "|{65e550ecb142680e...|[{-31493, , 2024-...|  META|\n",
      "|{65e55104b142680e...|[{-500, , 2024-02...|  AMZN|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load inside transactions from mongodb\n",
    "inside_trading_df = spark.read.format('mongo') \\\n",
    "    .option(\"database\", 'finance_api') \\\n",
    "    .option(\"collection\", 'finn_insider_transactions') \\\n",
    "    .load()\n",
    "\n",
    "inside_trading_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- change: long (nullable = true)\n",
      " |    |    |-- currency: string (nullable = true)\n",
      " |    |    |-- filingDate: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- isDerivative: boolean (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- share: long (nullable = true)\n",
      " |    |    |-- source: string (nullable = true)\n",
      " |    |    |-- symbol: string (nullable = true)\n",
      " |    |    |-- transactionCode: string (nullable = true)\n",
      " |    |    |-- transactionDate: string (nullable = true)\n",
      " |    |    |-- transactionPrice: double (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inside_trading_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------+-----------------+----------------+-------+\n",
      "|symbol|transactionDate|  share|             name|transactionPrice| change|\n",
      "+------+---------------+-------+-----------------+----------------+-------+\n",
      "|  AAPL|     2024-02-29|4434576|LEVINSON ARTHUR D|          180.94|-100000|\n",
      "|  AAPL|     2024-02-28|   1516|     WAGNER SUSAN|             0.0|   1516|\n",
      "|  AAPL|     2024-02-28|   1516|   SUGAR RONALD D|             0.0|   1516|\n",
      "|  AAPL|     2024-02-28|   1516|  LOZANO MONICA C|             0.0|   1516|\n",
      "|  AAPL|     2024-02-28|   1516|LEVINSON ARTHUR D|             0.0|   1516|\n",
      "|  AAPL|     2024-02-28|   1516|      JUNG ANDREA|             0.0|   1516|\n",
      "|  AAPL|     2024-02-28|   1516|      Gorsky Alex|             0.0|   1516|\n",
      "|  AAPL|     2024-02-28|   1516|   Austin Wanda M|             0.0|   1516|\n",
      "|  AAPL|     2024-02-01|      0|     WAGNER SUSAN|             0.0|  -1852|\n",
      "|  AAPL|     2024-02-01|  60975|     WAGNER SUSAN|             0.0|   1852|\n",
      "|  AAPL|     2024-02-01|      0|   SUGAR RONALD D|             0.0|  -1852|\n",
      "|  AAPL|     2024-02-01| 107795|   SUGAR RONALD D|             0.0|   1852|\n",
      "|  AAPL|     2024-02-01|      0|  LOZANO MONICA C|             0.0|  -1852|\n",
      "|  AAPL|     2024-02-01|   7091|  LOZANO MONICA C|             0.0|   1852|\n",
      "|  AAPL|     2024-02-01|      0|LEVINSON ARTHUR D|             0.0|  -1852|\n",
      "+------+---------------+-------+-----------------+----------------+-------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# flattern inside_trading schema\n",
    "inside_trading_new_df = inside_trading_df.select('symbol', explode('data').alias('trade_info')) \\\n",
    "                                    .select('symbol', 'trade_info.transactionDate', \n",
    "                                            'trade_info.share', 'trade_info.name',\n",
    "                                            'trade_info.transactionPrice', 'trade_info.change')\n",
    "# df_flat = df_flat.select('name', 'age', 'car_info.car1', 'car_info.model')\n",
    "\n",
    "inside_trading_new_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41787"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inside_trading_new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /user/input/project/fmp_company_profile\n",
    "profile_df = spark.read.csv('hdfs://localhost:9000/user/input/project/fmp_company_profile', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+-------+----------+----------+--------------------+-------+--------+---------+-------------------+---------+------------+--------------------+--------------------+-----------------+-----------------+--------------------+--------------------+----------+-----------------+-----+-----+------+------------+-------+-------------+------------+------+-------------+---------------+-----+------+--------+--------------------+----------+\n",
      "|             address| beta|                 ceo|changes|       cik|      city|         companyName|country|currency|    cusip|                dcf|  dcfDiff|defaultImage|         description|            exchange|exchangeShortName|fullTimeEmployees|               image|            industry|   ipoDate|isActivelyTrading|isAdr|isEtf|isFund|        isin|lastDiv|       mktCap|       phone| price|        range|         sector|state|symbol|  volAvg|             website|       zip|\n",
      "+--------------------+-----+--------------------+-------+----------+----------+--------------------+-------+--------+---------+-------------------+---------+------------+--------------------+--------------------+-----------------+-----------------+--------------------+--------------------+----------+-----------------+-----+-----+------+------------+-------+-------------+------------+------+-------------+---------------+-----+------+--------+--------------------+----------+\n",
      "|   301 Merritt Seven| 1.02|Mr. Anthony J. Guzzi|   6.47|0000105634|   Norwalk|   EMCOR Group, Inc.|     US|     USD|29084Q100| 0.2769617108422666|319.71304|       false|EMCOR Group, Inc....|New York Stock Ex...|             NYSE|            38300|https://financial...|Engineering & Con...|1995-01-10|             true|false|false| false|US29084Q1004|   0.72|  15053737556|203 849 7800|319.99|151.52-321.08|    Industrials|   CT|   EME|  360251|https://www.emcor...|06851-1092|\n",
      "|   One Microsoft Way|0.899|  Mr. Satya  Nadella|   1.86|0000789019|   Redmond|Microsoft Corpora...|     US|     USD|594918104| 324.03723609617424| 91.46276|       false|Microsoft Corpora...|NASDAQ Global Select|           NASDAQ|           221000|https://financial...|Software—Infrastr...|1986-03-13|             true|false|false| false|US5949181045|      3|3087347820000|425 882 8080| 415.5|245.73-420.82|     Technology|   WA|  MSFT|24358515|https://www.micro...|98052-6399|\n",
      "|50 Applied Bank Blvd| 1.52|Mr. Chrishan Anth...|  -0.25|0001616862|Glen Mills|Axalta Coating Sy...|     US|     USD|G0750C108| 25.346843310610744|  7.13316|       false|Axalta Coating Sy...|New York Stock Ex...|             NYSE|            12000|https://financial...| Specialty Chemicals|2014-11-12|             true|false|false| false|BMG0750C1082|      0|   7150147200|855 547 1461| 32.48|  25.03-34.45|Basic Materials|   PA|  AXTA| 2066368|https://www.axalt...|     19342|\n",
      "|7402 North Eldrid...|1.398|Mr. David A. Cher...|   0.04|0001599617|   Houston|            NOW Inc.|     US|     USD|67011P100|  12.98314576615849|  1.20685|       false|NOW Inc. distribu...|New York Stock Ex...|             NYSE|             2375|https://financial...|Oil & Gas Equipme...|2014-05-20|             true|false|false| false|US67011P1003|      0|   1506495540|281 823 4700| 14.19|  8.83-14.445|         Energy|   TX|  DNOW| 1073416|https://www.dnow.com|     77041|\n",
      "|  436 Seventh Avenue|1.869|Mr. Leroy Mangus ...|  -3.65|0001315257|Pittsburgh|Koppers Holdings ...|     US|     USD|50060P106|-17.341096609147165|  70.3111|       false|Koppers Holdings ...|New York Stock Ex...|             NYSE|             2119|https://financial...| Specialty Chemicals|2006-02-01|             true|false|false| false|US50060P1066|   0.28|   1102999607|412 227 2001| 52.97|  28.45-58.23|Basic Materials|   PA|   KOP|  142380|https://www.koppe...|15219-1800|\n",
      "+--------------------+-----+--------------------+-------+----------+----------+--------------------+-------+--------+---------+-------------------+---------+------------+--------------------+--------------------+-----------------+-----------------+--------------------+--------------------+----------+-----------------+-----+-----+------+------------+-------+-------------+------------+------+-------------+---------------+-----+------+--------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
