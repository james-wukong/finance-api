{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import col, to_date, to_timestamp, explode\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.errors import PySparkException\n",
    "from pymongo import MongoClient\n",
    "from pymongo import errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mongo_uri(mongo_conf) -> str | None:\n",
    "    \"\"\"\n",
    "    generate mongo connection uri based on input\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not mongo_conf:\n",
    "        return None\n",
    "    return (f\"mongodb+srv://{mongo_conf['user']}:\"\n",
    "            f\"{mongo_conf['token']}@\"\n",
    "            f\"{mongo_conf['host']}\"\n",
    "            f\"/?retryWrites=true&w=majority\")\n",
    "\n",
    "def get_property(conf) -> dict:\n",
    "    \"\"\"\n",
    "    get property for database\n",
    "    \"\"\"\n",
    "    return {key: conf[key] for key in conf.keys()\n",
    "                         & {'user', 'password', 'driver'}}\n",
    "\n",
    "def gen_maria_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for maria database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:mysql://{conf['host']}:\"\n",
    "            f\"{conf['port']}/\"\n",
    "            f\"{db}?permitMysqlScheme\")\n",
    "\n",
    "def gen_postgres_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for maria database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:postgresql://{conf['host']}:\"\n",
    "            f\"{conf['port']}/{db}\")\n",
    "\n",
    "def gen_mssql_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for maria database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:sqlserver://{conf['host']}:{conf['port']};\"\n",
    "            f\"databaseName={db};encrypt=true;trustServerCertificate=true;\")\n",
    "\n",
    "def gen_azure_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for azure sql database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:sqlserver://{conf['host']}:{conf['port']};\"\n",
    "            f\"databaseName={db};encrypt=true;trustServerCertificate=true;\")\n",
    "\n",
    "\n",
    "def init_mongodb_client(uri: str) -> MongoClient | None:\n",
    "    \"\"\"\n",
    "    initialize the mongo client\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize new MongoDB client\n",
    "        client = MongoClient(uri)\n",
    "    except errors.ConnectionFailure as e:\n",
    "        # Handle connection failure gracefully\n",
    "        print(f\"Failed to connect to MongoDB: {e}\")\n",
    "\n",
    "        return None\n",
    "    else:\n",
    "        return client\n",
    "\n",
    "def prepare_dataframe(spark, sc, data) -> DataFrame:\n",
    "    \"\"\"\n",
    "    prepare dataframe when the data might be in different data types\n",
    "    :param spark: spark session\n",
    "    :param sc: spark context\n",
    "    :param data: data to be processed\n",
    "    :return: spark sql dataframe\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame) and not data.empty:\n",
    "        df = spark.createDataFrame(data)\n",
    "    elif isinstance(data, DataFrame) and not data.isEmpty():\n",
    "        df = data\n",
    "    elif isinstance(data, list):\n",
    "        df = spark.read.json(sc.parallelize([json.dumps(record) for record in data]))\n",
    "    elif isinstance(data, dict):\n",
    "        data = [data]\n",
    "        df = spark.read.json(sc.parallelize([json.dumps(record) for record in data]))\n",
    "    else:\n",
    "        # initialize empty dataframe\n",
    "        schema = StructType([])\n",
    "        df = spark.createDataFrame([], schema)\n",
    "\n",
    "    return df\n",
    "    \n",
    "def write_to_mongo(spark, data, \n",
    "                   uri: str, db: str, col: str) -> None:\n",
    "    \"\"\"\n",
    "    write data into mongo db\n",
    "    :param spark: sparkSession\n",
    "    :param data: \n",
    "    :param uri: str, \n",
    "    :param db: str, \n",
    "    :param col: str, \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sc = spark.sparkContext\n",
    "    df = prepare_dataframe(spark, sc, data)\n",
    "    config = {\n",
    "        'uri': uri,\n",
    "        'database': db,\n",
    "        'collection': col\n",
    "    }\n",
    "    if not df.isEmpty():\n",
    "        df.write.format(\"mongo\") \\\n",
    "            .options(**config) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "\n",
    "def write_to_database(spark, data, conf,\n",
    "                   db: str, write_table: str, type: str = 'mariadb') -> None:\n",
    "    \"\"\"\n",
    "    write data into database table\n",
    "    :param data:\n",
    "    :param uri:\n",
    "    :param db:\n",
    "    :param table:\n",
    "    :param type:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sc = spark.sparkContext\n",
    "    df = prepare_dataframe(spark, sc, data)\n",
    "    properties = get_property(conf)\n",
    "\n",
    "    jdbc = ''\n",
    "    if type == 'mariadb':\n",
    "        jdbc = gen_maria_jdbc(conf, db)\n",
    "    elif type == 'postgres':\n",
    "        jdbc = gen_postgres_jdbc(conf, db)\n",
    "    elif type == 'mssql':\n",
    "        jdbc = gen_mssql_jdbc(conf, db)\n",
    "    elif type == 'azure':\n",
    "        jdbc = gen_azure_jdbc(conf, db)\n",
    "    else:\n",
    "        raise ValueError('error when generating jdbc')\n",
    "\n",
    "    if not df.isEmpty():\n",
    "        df.write.jdbc(\n",
    "            url=jdbc,\n",
    "            table=write_table,\n",
    "            mode=\"append\",\n",
    "            properties=properties\n",
    "        )\n",
    "    else:\n",
    "        print('empty dataset')\n",
    "\n",
    "def sentiment_analysis(data, tokenizer, model) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    sentiment analysis and retrun dataframe with score\n",
    "    \"\"\"\n",
    "    data['sentiment'] = None\n",
    "    for idx, text in enumerate(data['text']):\n",
    "        tokenized_news = tokenizer(text, truncation=True, return_tensors=\"tf\")\n",
    "        logits = model.predict(tokenized_news).logits\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "        data.at[idx, 'sentiment'] = probabilities.numpy()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('conf.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "mongo_uri = gen_mongo_uri(config['mongodb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/sutring/.ivy2/cache\n",
      "The jars for the packages stored in: /home/sutring/.ivy2/jars\n",
      "com.microsoft.azure#spark-mssql-connector_2.12 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ce3c68d6-5d57-462a-b0ac-188c1e0c1e53;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.microsoft.azure#spark-mssql-connector_2.12;1.2.0 in central\n",
      "\tfound com.microsoft.sqlserver#mssql-jdbc;8.4.1.jre8 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 271ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.microsoft.azure#spark-mssql-connector_2.12;1.2.0 from central in [default]\n",
      "\tcom.microsoft.sqlserver#mssql-jdbc;8.4.1.jre8 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ce3c68d6-5d57-462a-b0ac-188c1e0c1e53\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/8ms)\n",
      "24/03/10 17:59:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # getting the spark instance\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName('Big Data Project ETL') \\\n",
    "        .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
    "        .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
    "        .config(\"spark.jars.packages\", \"com.microsoft.azure:spark-mssql-connector_2.12:1.2.0,\"\n",
    "                                       \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "except PySparkException as e:\n",
    "    print(f\"Failed to get or create Spark: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Exception Caught: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read historical data with sentiment from mariadb\n",
    "historical_stock_df = spark.read.jdbc(\n",
    "    url=gen_maria_jdbc(config['mariadb'], 'finance_out'),\n",
    "    table='historical_with_sentiment',\n",
    "    properties=get_property(config['mariadb'])\n",
    ")\n",
    "# read commitment trader report data from mssql\n",
    "cot_report_df = spark.read.jdbc(\n",
    "    url=gen_postgres_jdbc(config['postgres'], 'finance_api'),\n",
    "    table='fmp_cmtmt_trader_report',\n",
    "    properties=get_property(config['postgres'])\n",
    ")\n",
    "# read income statement data from postgresql\n",
    "income_stmt_df = spark.read.jdbc(\n",
    "    url=gen_postgres_jdbc(config['postgres'], 'finance_api'),\n",
    "    table='fmp_income_stmt',\n",
    "    properties=get_property(config['postgres'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------------+----------+---------+\n",
      "|symbol|      date|              Close|    Volume|sentiment|\n",
      "+------+----------+-------------------+----------+---------+\n",
      "|  AAPL|2004-03-03|0.42714300751686096| 225131200|     NULL|\n",
      "|  AAPL|2004-03-04|0.44928601384162903| 660223200|     NULL|\n",
      "|  AAPL|2004-03-05|0.47749999165534973|1540599200|     NULL|\n",
      "|  AAPL|2004-03-08| 0.4642859995365143| 522872000|     NULL|\n",
      "|  AAPL|2004-03-09|0.48392900824546814| 618363200|     NULL|\n",
      "+------+----------+-------------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 116277)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_stock_df.show(5), historical_stock_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+--------------------------+---------------------------+-----------------------+------------------------+-----------+---------------+\n",
      "|               date|symbol|noncomm_positions_long_all|noncomm_positions_short_all|comm_positions_long_all|comm_positions_short_all|comm_spread|non_comm_spread|\n",
      "+-------------------+------+--------------------------+---------------------------+-----------------------+------------------------+-----------+---------------+\n",
      "|2024-02-27 00:00:00|    BT|                     20034|                      22001|                   1759|                     799|        960|          -1967|\n",
      "|2024-02-20 00:00:00|    BT|                     19624|                      21722|                   1862|                     660|       1202|          -2098|\n",
      "|2024-02-13 00:00:00|    BT|                     19147|                      21068|                   1514|                     362|       1152|          -1921|\n",
      "|2024-02-06 00:00:00|    BT|                     16280|                      17803|                   1237|                     480|        757|          -1523|\n",
      "|2024-01-30 00:00:00|    BT|                     16519|                      18317|                   1253|                     236|       1017|          -1798|\n",
      "+-------------------+------+--------------------------+---------------------------+-----------------------+------------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 9775)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cot_report_df.show(5), cot_report_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/10 18:00:12 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+----------+---------------+-------------+----------+---------------------------+---------+------------+------+----------+-----------+--------------------+--------------------------------+-----------+----------------+---------------+--------------------+----------------+---------------+--------------+--------------------+---------+--------------+-----------------+---------------+--------------------+-------------+------+----------------+------------------------------+----------+---------------------------+---------------------------------------+------+---------------------------+---------------------+------------------------+\n",
      "|       acceptedDate|calendarYear|       cik|costAndExpenses|costOfRevenue|      date|depreciationAndAmortization|   ebitda| ebitdaratio|   eps|epsdiluted|fillingDate|           finalLink|generalAndAdministrativeExpenses|grossProfit|grossProfitRatio|incomeBeforeTax|incomeBeforeTaxRatio|incomeTaxExpense|interestExpense|interestIncome|                link|netIncome|netIncomeRatio|operatingExpenses|operatingIncome|operatingIncomeRatio|otherExpenses|period|reportedCurrency|researchAndDevelopmentExpenses|   revenue|sellingAndMarketingExpenses|sellingGeneralAndAdministrativeExpenses|symbol|totalOtherIncomeExpensesNet|weightedAverageShsOut|weightedAverageShsOutDil|\n",
      "+-------------------+------------+----------+---------------+-------------+----------+---------------------------+---------+------------+------+----------+-----------+--------------------+--------------------------------+-----------+----------------+---------------+--------------------+----------------+---------------+--------------+--------------------+---------+--------------+-----------------+---------------+--------------------+-------------+------+----------------+------------------------------+----------+---------------------------+---------------------------------------+------+---------------------------+---------------------+------------------------+\n",
      "|2004-12-02 18:05:49|        2004|0000320193|     2217000000|   1716000000|2004-09-25|                   40000000|192000000|0.0817021277|0.0048|    0.0045| 2004-12-03|https://www.sec.g...|                               0|  634000000|     0.269787234|      147000000|        0.0625531915|        41000000|        5000000|      20000000|https://www.sec.g...|106000000|   0.045106383|        501000000|      128000000|        0.0544680851|     -1000000|    Q4|             USD|                     122000000|2350000000|                          0|                              379000000|  AAPL|                   19000000|          22092896000|             23468872000|\n",
      "|2004-08-04 20:48:51|        2004|0000320193|     1934000000|   1455000000|2004-06-26|                   41000000|133000000|0.0660377358|0.0029|    0.0029| 2004-08-05|https://www.sec.g...|                               0|  559000000|    0.2775571003|       85000000|         0.042204568|        24000000|        7000000|      15000000|https://www.sec.g...| 61000000|  0.0302879841|        479000000|       72000000|        0.0357497517|     -3000000|    Q3|             USD|                     125000000|2014000000|                          0|                              354000000|  AAPL|                   13000000|          21001288000|             21986776000|\n",
      "|2004-05-06 17:26:09|        2004|0000320193|     1847000000|   1379000000|2004-03-27|                   36000000|111000000| 0.058145626|0.0021|    0.0021| 2004-05-06|https://www.sec.g...|                               0|  530000000|    0.2776322682|       64000000|         0.033525406|        18000000|        1000000|      15000000|https://www.sec.g...| 46000000|  0.0240963855|        468000000|       75000000|        0.0392875851|     -2000000|    Q2|             USD|                     123000000|1909000000|                          0|                              345000000|  AAPL|                  -11000000|          20459432000|             21180880000|\n",
      "|2004-02-09 19:57:13|        2004|0000320193|     1932000000|   1470000000|2003-12-27|                   33000000|118000000|0.0588235294| 0.003|     0.003| 2004-02-10|https://www.sec.g...|                               0|  536000000|    0.2671984048|       87000000|        0.0433698903|        24000000|        2000000|      14000000|https://www.sec.g...| 63000000|  0.0314057827|        462000000|       74000000|         0.036889332|     -3000000|    Q1|             USD|                     119000000|2006000000|                          0|                              343000000|  AAPL|                   13000000|          20297200000|             20849248000|\n",
      "|2003-12-19 17:25:45|        2003|0000320193|     1684000000|   1259000000|2003-09-27|                   20000000| 63000000|0.0367346939|0.0022|    0.0021| 2003-12-19|https://www.sec.g...|                               0|  456000000|    0.2658892128|       55000000|        0.0320699708|        14000000|        2000000|      13000000|https://www.sec.g...| 44000000|  0.0256559767|        425000000|       43000000|        0.0250728863|     -1000000|    Q4|             USD|                     111000000|1715000000|                          0|                              314000000|  AAPL|                   12000000|          20297200000|             20849248000|\n",
      "+-------------------+------------+----------+---------------+-------------+----------+---------------------------+---------+------------+------+----------+-----------+--------------------+--------------------------------+-----------+----------------+---------------+--------------------+----------------+---------------+--------------+--------------------+---------+--------------+-----------------+---------------+--------------------+-------------+------+----------------+------------------------------+----------+---------------------------+---------------------------------------+------+---------------------------+---------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 3026)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_stmt_df.show(5), income_stmt_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get unique symbols\n",
    "stock_symbols = historical_stock_df.select(\"symbol\").distinct().rdd.flatMap(lambda x : x).collect()\n",
    "sec_symbols = cot_report_df.select(\"symbol\").distinct().rdd.flatMap(lambda x : x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save historical data into postgresql\n",
    "for symbol in stock_symbols:\n",
    "    stock_data = historical_stock_df.filter(historical_stock_df['symbol'] == symbol) \\\n",
    "                                .select('date', 'Close', 'sentiment') \\\n",
    "                                .withColumnRenamed('Close', symbol)\n",
    "    try:\n",
    "        # save with 'stock_' as prefix\n",
    "        write_to_database(spark, stock_data, config['postgres'],\n",
    "                          db='local_finance_stage', write_table=f'stock_{symbol}', type='postgres')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save commitment report data into postgresql\n",
    "for symbol in sec_symbols:\n",
    "    cot_data = cot_report_df.filter(cot_report_df['symbol'] == symbol) \\\n",
    "                            .withColumn('date', to_date('date')) \\\n",
    "                            .select(\"date\", \"symbol\", \"comm_spread\", \"non_comm_spread\")\n",
    "    try:\n",
    "        # save with 'stock_' as prefix\n",
    "        write_to_database(spark, cot_data, config['postgres'],\n",
    "                          db='local_finance_stage', write_table=f'cot_report_{symbol}', type='postgres')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save income statement data into postgresql\n",
    "for symbol in stock_symbols:\n",
    "    income_stmt_data = income_stmt_df.filter(income_stmt_df['symbol'] == \"AAPL\") \\\n",
    "                                        .drop('date') \\\n",
    "                                        .withColumn('acceptedDate', \n",
    "                                                    income_stmt_df.acceptedDate.cast(\"date\").alias('date')) \\\n",
    "                                        .withColumnRenamed('acceptedDate', 'date')\n",
    "    try:\n",
    "        # save with 'stock_' as prefix\n",
    "        write_to_database(spark, income_stmt_data, config['postgres'],\n",
    "                          db='local_finance_stage', write_table=f'income_stmt_{symbol}', type='postgres')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_profile = spark.read.csv(\"hdfs://localhost:9000/user/input/project/fmp_company_profile\", header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save company profile in postgresql\n",
    "try:\n",
    "    write_to_database(spark, company_profile, config['postgres'],\n",
    "                      db='local_finance_stage', write_table=f'stock_company_profile', type='postgres')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load state crime data from postgresql\n",
    "# state_crime_rate_df = spark.read.jdbc(\n",
    "#     url=gen_postgres_jdbc(config['postgres'], 'finance_out'),\n",
    "#     table='state_crime_rate',\n",
    "#     properties=get_property(config['postgres'])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save state crime rate in mssql or azure sql\n",
    "# try:\n",
    "#     # save with 'stock_' as prefix\n",
    "#     write_to_database(spark, state_crime_rate_df, config['postgres'],\n",
    "#                       db='local_finance_stage', write_table='state_crime_rate', type='postgres')\n",
    "# except Exception as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|                 _id|                data|symbol|\n",
      "+--------------------+--------------------+------+\n",
      "|{65e550a0b142680e...|[{-100000, , 2024...|  AAPL|\n",
      "|{65e550bab142680e...|[{-300, , 2024-02...|  MSFT|\n",
      "|{65e550d4b142680e...|[{-5000, , 2024-0...|  NVDA|\n",
      "|{65e550ecb142680e...|[{-31493, , 2024-...|  META|\n",
      "|{65e55104b142680e...|[{-500, , 2024-02...|  AMZN|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load inside transactions from mongodb\n",
    "inside_trading_df = spark.read.format('mongo') \\\n",
    "    .option(\"database\", 'finance_api') \\\n",
    "    .option(\"collection\", 'finn_insider_transactions') \\\n",
    "    .load()\n",
    "\n",
    "inside_trading_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- change: long (nullable = true)\n",
      " |    |    |-- currency: string (nullable = true)\n",
      " |    |    |-- filingDate: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- isDerivative: boolean (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- share: long (nullable = true)\n",
      " |    |    |-- source: string (nullable = true)\n",
      " |    |    |-- symbol: string (nullable = true)\n",
      " |    |    |-- transactionCode: string (nullable = true)\n",
      " |    |    |-- transactionDate: string (nullable = true)\n",
      " |    |    |-- transactionPrice: double (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inside_trading_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------+-----------------+----------------+-------+\n",
      "|symbol|transactionDate|  share|             name|transactionPrice| change|\n",
      "+------+---------------+-------+-----------------+----------------+-------+\n",
      "|  AAPL|     2024-02-29|4434576|LEVINSON ARTHUR D|          180.94|-100000|\n",
      "|  AAPL|     2024-02-28|   1516|     WAGNER SUSAN|             0.0|   1516|\n",
      "|  AAPL|     2024-02-28|   1516|   SUGAR RONALD D|             0.0|   1516|\n",
      "|  AAPL|     2024-02-28|   1516|  LOZANO MONICA C|             0.0|   1516|\n",
      "|  AAPL|     2024-02-28|   1516|LEVINSON ARTHUR D|             0.0|   1516|\n",
      "|  AAPL|     2024-02-28|   1516|      JUNG ANDREA|             0.0|   1516|\n",
      "|  AAPL|     2024-02-28|   1516|      Gorsky Alex|             0.0|   1516|\n",
      "|  AAPL|     2024-02-28|   1516|   Austin Wanda M|             0.0|   1516|\n",
      "|  AAPL|     2024-02-01|      0|     WAGNER SUSAN|             0.0|  -1852|\n",
      "|  AAPL|     2024-02-01|  60975|     WAGNER SUSAN|             0.0|   1852|\n",
      "|  AAPL|     2024-02-01|      0|   SUGAR RONALD D|             0.0|  -1852|\n",
      "|  AAPL|     2024-02-01| 107795|   SUGAR RONALD D|             0.0|   1852|\n",
      "|  AAPL|     2024-02-01|      0|  LOZANO MONICA C|             0.0|  -1852|\n",
      "|  AAPL|     2024-02-01|   7091|  LOZANO MONICA C|             0.0|   1852|\n",
      "|  AAPL|     2024-02-01|      0|LEVINSON ARTHUR D|             0.0|  -1852|\n",
      "+------+---------------+-------+-----------------+----------------+-------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# flattern inside_trading schema\n",
    "inside_trading_new_df = inside_trading_df.select('symbol', explode('data').alias('trade_info')) \\\n",
    "                                    .select('symbol', 'trade_info.transactionDate', \n",
    "                                            'trade_info.share', 'trade_info.name',\n",
    "                                            'trade_info.transactionPrice', 'trade_info.change')\n",
    "# df_flat = df_flat.select('name', 'age', 'car_info.car1', 'car_info.model')\n",
    "\n",
    "inside_trading_new_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41787"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inside_trading_new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save inside trading in postgresql\n",
    "try:\n",
    "    write_to_database(spark, inside_trading_new_df, config['postgres'],\n",
    "                      db='local_finance_stage', write_table=f'stock_inside_trading', type='postgres')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save housing data into postgresql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling null values in back fill way\n",
    "housing_ps = pd.read_csv('data/House_Price_month.csv')\n",
    "housing_ps.drop(columns=['RegionID', 'SizeRank', 'RegionName', 'RegionType', 'StateName'], inplace=True)\n",
    "housing_ps = housing_ps.T.bfill().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = spark.createDataFrame(housing_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save stage housing price in postgresql\n",
    "try:\n",
    "    write_to_database(spark, housing_df, config['postgres'],\n",
    "                      db='local_finance_stage', write_table=f'state_housing_price', type='postgres')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'LOGI', 'XOM', 'BMY', 'META', 'LNG', 'TSLA', 'EME', 'SWN', 'AXTA', 'CL', 'SBLK', 'CVX', 'TNDM', 'GTLB', 'GOOGL', 'APA', 'ON', 'MOS', 'DBD', 'DSGX', 'KOP', 'BTU', 'THC', 'AMZN', 'MSFT', 'CRSR', 'DNOW', 'NVDA']\n"
     ]
    }
   ],
   "source": [
    "print(stock_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- processing AAPL --------------------\n",
      "-------------------- AAPL complete --------------------\n",
      "-------------------- processing LOGI --------------------\n",
      "-------------------- LOGI complete --------------------\n",
      "-------------------- processing XOM --------------------\n",
      "-------------------- XOM complete --------------------\n",
      "-------------------- processing BMY --------------------\n",
      "-------------------- BMY complete --------------------\n",
      "-------------------- processing META --------------------\n",
      "-------------------- META complete --------------------\n",
      "-------------------- processing LNG --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- LNG complete --------------------\n",
      "-------------------- processing TSLA --------------------\n",
      "-------------------- TSLA complete --------------------\n",
      "-------------------- processing EME --------------------\n",
      "-------------------- EME complete --------------------\n",
      "-------------------- processing SWN --------------------\n",
      "-------------------- SWN complete --------------------\n",
      "-------------------- processing AXTA --------------------\n",
      "-------------------- AXTA complete --------------------\n",
      "-------------------- processing CL --------------------\n",
      "-------------------- CL complete --------------------\n",
      "-------------------- processing SBLK --------------------\n",
      "-------------------- SBLK complete --------------------\n",
      "-------------------- processing CVX --------------------\n",
      "-------------------- CVX complete --------------------\n",
      "-------------------- processing TNDM --------------------\n",
      "-------------------- TNDM complete --------------------\n",
      "-------------------- processing GTLB --------------------\n",
      "-------------------- GTLB complete --------------------\n",
      "-------------------- processing GOOGL --------------------\n",
      "-------------------- GOOGL complete --------------------\n",
      "-------------------- processing APA --------------------\n",
      "-------------------- APA complete --------------------\n",
      "-------------------- processing ON --------------------\n",
      "-------------------- ON complete --------------------\n",
      "-------------------- processing MOS --------------------\n",
      "-------------------- MOS complete --------------------\n",
      "-------------------- processing DBD --------------------\n",
      "-------------------- DBD complete --------------------\n",
      "-------------------- processing DSGX --------------------\n",
      "-------------------- DSGX complete --------------------\n",
      "-------------------- processing KOP --------------------\n",
      "-------------------- KOP complete --------------------\n",
      "-------------------- processing BTU --------------------\n",
      "-------------------- BTU complete --------------------\n",
      "-------------------- processing THC --------------------\n",
      "-------------------- THC complete --------------------\n",
      "-------------------- processing AMZN --------------------\n",
      "-------------------- AMZN complete --------------------\n",
      "-------------------- processing MSFT --------------------\n",
      "-------------------- MSFT complete --------------------\n",
      "-------------------- processing CRSR --------------------\n",
      "-------------------- CRSR complete --------------------\n",
      "-------------------- processing DNOW --------------------\n",
      "-------------------- DNOW complete --------------------\n",
      "-------------------- processing NVDA --------------------\n",
      "-------------------- NVDA complete --------------------\n"
     ]
    }
   ],
   "source": [
    "# transfer tables from local respository to azure sql\n",
    "# transfer historical data and income statement\n",
    "for symbol in stock_symbols:\n",
    "    print(f'{\"-\" * 20} processing {symbol} {\"-\" * 20}')\n",
    "    income_stmt = spark.read.jdbc(\n",
    "        url=gen_postgres_jdbc(config['postgres'], 'local_finance_stage'),\n",
    "        table=f'income_stmt_{symbol}',\n",
    "        properties=get_property(config['postgres'])\n",
    "    )\n",
    "    try:\n",
    "        write_to_database(spark, income_stmt, config['azuresql'],\n",
    "                          db='final_finance_stage', write_table=f'income_stmt_{symbol}', type='azure')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    stock_data = spark.read.jdbc(\n",
    "        url=gen_postgres_jdbc(config['postgres'], 'local_finance_stage'),\n",
    "        table=f'stock_{symbol}',\n",
    "        properties=get_property(config['postgres'])\n",
    "    )\n",
    "    try:\n",
    "        write_to_database(spark, stock_data, config['azuresql'],\n",
    "                          db='final_finance_stage', write_table=f'stock_{symbol}', type='azure')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    print(f'{\"-\" * 20} {symbol} complete {\"-\" * 20}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- processing ZN --------------------\n",
      "-------------------- ZN complete --------------------\n",
      "-------------------- processing DX --------------------\n",
      "-------------------- DX complete --------------------\n",
      "-------------------- processing CL --------------------\n",
      "-------------------- CL complete --------------------\n",
      "-------------------- processing ES --------------------\n",
      "-------------------- ES complete --------------------\n",
      "-------------------- processing GC --------------------\n",
      "-------------------- GC complete --------------------\n",
      "-------------------- processing LS --------------------\n",
      "-------------------- LS complete --------------------\n",
      "-------------------- processing BT --------------------\n",
      "-------------------- BT complete --------------------\n",
      "-------------------- processing SI --------------------\n",
      "-------------------- SI complete --------------------\n",
      "-------------------- processing NG --------------------\n",
      "-------------------- NG complete --------------------\n"
     ]
    }
   ],
   "source": [
    "# transfer commitment trader report\n",
    "for symbol in sec_symbols:\n",
    "    print(f'{\"-\" * 20} processing {symbol} {\"-\" * 20}')\n",
    "    cot_data = spark.read.jdbc(\n",
    "        url=gen_postgres_jdbc(config['postgres'], 'local_finance_stage'),\n",
    "        table=f'cot_report_{symbol}',\n",
    "        properties=get_property(config['postgres'])\n",
    "    )\n",
    "    try:\n",
    "        write_to_database(spark, cot_data, config['azuresql'],\n",
    "                          db='final_finance_stage', write_table=f'cot_report_{symbol}', type='azure')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    print(f'{\"-\" * 20} {symbol} complete {\"-\" * 20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer company profile data\n",
    "company_profile = spark.read.jdbc(\n",
    "    url=gen_postgres_jdbc(config['postgres'], 'local_finance_stage'),\n",
    "    table=f'stock_company_profile',\n",
    "    properties=get_property(config['postgres'])\n",
    ")\n",
    "try:\n",
    "    write_to_database(spark, stock_data, config['azuresql'],\n",
    "                      db='final_finance_stage', write_table=f'stock_company_profile', type='azure')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer state crime data\n",
    "state_crime = spark.read.jdbc(\n",
    "    url=gen_postgres_jdbc(config['postgres'], 'local_finance_stage'),\n",
    "    table=f'state_crime_rate',\n",
    "    properties=get_property(config['postgres'])\n",
    ")\n",
    "try:\n",
    "    write_to_database(spark, state_crime, config['azuresql'],\n",
    "                      db='final_finance_stage', write_table=f'state_crime_rate', type='azure')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# transfer statehousing data\n",
    "state_housing = spark.read.jdbc(\n",
    "    url=gen_postgres_jdbc(config['postgres'], 'local_finance_stage'),\n",
    "    table=f'state_housing_price',\n",
    "    properties=get_property(config['postgres'])\n",
    ")\n",
    "try:\n",
    "    write_to_database(spark, state_housing, config['azuresql'],\n",
    "                      db='final_finance_stage', write_table=f'state_housing_price', type='azure')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer stock inside trading\n",
    "inside_trading = spark.read.jdbc(\n",
    "    url=gen_postgres_jdbc(config['postgres'], 'local_finance_stage'),\n",
    "    table=f'stock_inside_trading',\n",
    "    properties=get_property(config['postgres'])\n",
    ")\n",
    "try:\n",
    "    write_to_database(spark, stock_data, config['azuresql'],\n",
    "                      db='final_finance_stage', write_table=f'stock_inside_trading', type='azure')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
