{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import col, to_date, to_timestamp\n",
    "from pyspark.errors import PySparkException\n",
    "from pymongo import MongoClient\n",
    "from pymongo import errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mongo_uri(mongo_conf) -> str | None:\n",
    "    \"\"\"\n",
    "    generate mongo connection uri based on input\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not mongo_conf:\n",
    "        return None\n",
    "    return (f\"mongodb+srv://{mongo_conf['user']}:\"\n",
    "            f\"{mongo_conf['token']}@\"\n",
    "            f\"{mongo_conf['host']}\"\n",
    "            f\"/?retryWrites=true&w=majority\")\n",
    "\n",
    "def get_property(conf) -> dict:\n",
    "    \"\"\"\n",
    "    get property for database\n",
    "    \"\"\"\n",
    "    return {key: conf[key] for key in conf.keys()\n",
    "                         & {'user', 'password', 'driver'}}\n",
    "\n",
    "def gen_maria_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for maria database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:mysql://{conf['host']}:\"\n",
    "            f\"{conf['port']}/\"\n",
    "            f\"{db}?permitMysqlScheme\")\n",
    "\n",
    "def gen_postgres_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for maria database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:postgresql://{conf['host']}:\"\n",
    "            f\"{conf['port']}/{db}\")\n",
    "\n",
    "def gen_mssql_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for maria database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:sqlserver://{conf['host']}:{conf['port']};\"\n",
    "            f\"databaseName={db};encrypt=true;trustServerCertificate=true;\")\n",
    "\n",
    "def gen_azure_jdbc(conf, db: str = '') -> str | None:\n",
    "    \"\"\"\n",
    "    get connection jdbc string for azure sql database\n",
    "    \"\"\"\n",
    "    if not conf:\n",
    "        return None\n",
    "    db = conf['database'] if not db else db\n",
    "    return (f\"jdbc:sqlserver://{conf['host']}:{conf['port']};\"\n",
    "            f\"databaseName={db};encrypt=true;\")\n",
    "\n",
    "\n",
    "def init_mongodb_client(uri: str) -> MongoClient | None:\n",
    "    \"\"\"\n",
    "    initialize the mongo client\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize new MongoDB client\n",
    "        client = MongoClient(uri)\n",
    "    except errors.ConnectionFailure as e:\n",
    "        # Handle connection failure gracefully\n",
    "        print(f\"Failed to connect to MongoDB: {e}\")\n",
    "\n",
    "        return None\n",
    "    else:\n",
    "        return client\n",
    "\n",
    "def prepare_dataframe(spark, sc, data) -> DataFrame:\n",
    "    \"\"\"\n",
    "    prepare dataframe when the data might be in different data types\n",
    "    :param spark: spark session\n",
    "    :param sc: spark context\n",
    "    :param data: data to be processed\n",
    "    :return: spark sql dataframe\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame) and not data.empty:\n",
    "        print('data is pandas df and not empty')\n",
    "        df = spark.createDataFrame(data)\n",
    "    elif isinstance(data, DataFrame) and not data.isEmpty():\n",
    "        print('data is spark df and not empty')\n",
    "        df = data\n",
    "    elif isinstance(data, list):\n",
    "        print('data is list and not empty')\n",
    "        df = spark.read.json(sc.parallelize([json.dumps(record) for record in data]))\n",
    "    elif isinstance(data, dict):\n",
    "        print('data is dictionary and not empty')\n",
    "        data = [data]\n",
    "        df = spark.read.json(sc.parallelize([json.dumps(record) for record in data]))\n",
    "    else:\n",
    "        \n",
    "        print('data empty')\n",
    "        # initialize empty dataframe\n",
    "        schema = StructType([])\n",
    "        df = spark.createDataFrame([], schema)\n",
    "\n",
    "    return df\n",
    "    \n",
    "def write_to_mongo(spark, data, \n",
    "                   uri: str, db: str, col: str) -> None:\n",
    "    \"\"\"\n",
    "    write data into mongo db\n",
    "    :param spark: sparkSession\n",
    "    :param data: \n",
    "    :param uri: str, \n",
    "    :param db: str, \n",
    "    :param col: str, \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sc = spark.sparkContext\n",
    "    df = prepare_dataframe(spark, sc, data)\n",
    "    config = {\n",
    "        'uri': uri,\n",
    "        'database': db,\n",
    "        'collection': col\n",
    "    }\n",
    "    if not df.isEmpty():\n",
    "        print('------------------writing data to mongodb----------------------')\n",
    "        df.write.format(\"mongo\") \\\n",
    "            .options(**config) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        print('------------------writing data to mongodb complete----------------------')\n",
    "\n",
    "def write_to_database(spark, data, conf,\n",
    "                   db: str, write_table: str, type: str = 'mariadb') -> None:\n",
    "    \"\"\"\n",
    "    write data into database table\n",
    "    :param data:\n",
    "    :param uri:\n",
    "    :param db:\n",
    "    :param table:\n",
    "    :param type:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sc = spark.sparkContext\n",
    "    df = prepare_dataframe(spark, sc, data)\n",
    "    properties = get_property(conf)\n",
    "\n",
    "    jdbc = ''\n",
    "    if type == 'mariadb':\n",
    "        jdbc = gen_maria_jdbc(conf, db)\n",
    "    elif type == 'postgres':\n",
    "        jdbc = gen_postgres_jdbc(conf, db)\n",
    "    elif type == 'mssql':\n",
    "        jdbc = gen_maria_jdbc(conf, db)\n",
    "    else:\n",
    "        raise ValueError('error when generating jdbc')\n",
    "\n",
    "    if not df.isEmpty():\n",
    "        print(f'------------------writing data to {type}----------------------')\n",
    "        df.write.jdbc(\n",
    "            url=jdbc,\n",
    "            table=write_table,\n",
    "            mode=\"append\",\n",
    "            properties=properties\n",
    "        )\n",
    "        print(f'------------------writing data to {type} complete----------------------')\n",
    "    else:\n",
    "        print('empty dataset')\n",
    "\n",
    "def sentiment_analysis(data, tokenizer, model) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    sentiment analysis and retrun dataframe with score\n",
    "    \"\"\"\n",
    "    data['sentiment'] = None\n",
    "    print('------------------predicting sentiments----------------------')\n",
    "    for idx, text in enumerate(data['text']):\n",
    "        tokenized_news = tokenizer(text, truncation=True, return_tensors=\"tf\")\n",
    "        logits = model.predict(tokenized_news).logits\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "        data.at[idx, 'sentiment'] = probabilities.numpy()\n",
    "    print('------------------predicting sentiments complete----------------------')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('conf.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "mongo_uri = gen_mongo_uri(config['mongodb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # getting the spark instance\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName('Big Data Project ETL') \\\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
    "        .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
    "        .config(\"spark.jars.packages\", \"com.microsoft.azure:spark-mssql-connector_2.12:1.2.0,\"\n",
    "                                       \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .getOrCreate()\n",
    "except PySparkException as e:\n",
    "    print(f\"Failed to get or create Spark: {e}\")\n",
    "except Exception as e:\n",
    "    print(f'Exception Caught: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_symbols = [\n",
    "        'AAPL', 'MSFT', 'NVDA', 'META', 'AMZN', 'TSLA', 'GOOGL',\n",
    "        'ON', 'DBD', 'DSGX', 'GTLB', 'LOGI', 'CRSR',\n",
    "        'LNG', 'SWN', 'APA', 'BTU', 'CL',\n",
    "        'BMY', 'THC', 'TNDM',\n",
    "        'MOS', 'AXTA', 'KOP',\n",
    "        'SBLK', 'EME', 'DNOW',\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load historical data from database\n",
    "maria_hist_data_df = spark.read.jdbc(\n",
    "    url=gen_maria_jdbc(config['mariadb']),\n",
    "    table='yf_historical_data',\n",
    "    properties=get_property(config['mariadb'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentiment data from csv\n",
    "news_sp_df = spark.read.csv(os.path.join('data/news_sentiment_all.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join news_sp_df with historical data\n",
    "joined_df = maria_hist_data_df.join(news_sp_df, on=['symbol', 'date'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write result data into mariadb\n",
    "write_to_database(spark, joined_df, config['mariadb'],\n",
    "                   db='finance_out', write_table='historical_with_sentiment', type = 'mariadb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
